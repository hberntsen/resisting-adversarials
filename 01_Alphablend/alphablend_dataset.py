""" This file creates a hdf5 dataset from the files generated by blender

This script opens the h5 file from each individual blender model and increases
the contrast using the range_extend function. From the resulting images it
creates datasets for testing and training. For each kind it generates 2
versions: one with a black background and the other with ImageNet backgrounds.
"""
import sys
sys.path.append('../')
import lib.database as db
import lib.backgroundproviders as bgp
import lib.alphablend as alphablend
import numpy as np
import random
from lib.range_extend import range_extend_min_max, range_extend

def generate_dataset(original_db, background_generator, size):
    d = db.empty_dataset(size)
    d['alphas'] = original_db['alphas']
    d['params'] = original_db['params']
    d['label'] = original_db['label']
    d['label_split'] = original_db['label_split']

    for i, background, image, alpha in zip(
            range(size), background_generator, original_db['images'], original_db['alphas']):
        d['images'][i] = alphablend.blend_background(image, alpha, background)
    return d

def empty_dataset(size=0):
    return {'alphas': np.empty((size,64*64)),
            'params': np.empty((size,3)),
            'label' : np.empty((size),dtype='uint8'),
            'label_split' : np.empty((size, 3),dtype='uint8'),
            'background_indexes' : np.empty((size)),
            'images': np.empty((size,1,64, 64), dtype='uint8')}

def append(loaded_datasets):
    return reduce(db.append2, loaded_datasets, empty_dataset())

def gen(npslice, output_type, black_filename, imagenet_filename):
    sets = ['suzanne','tux', 'airplane']
    imagenet = []
    black = []
    for i, s in enumerate(sets):
        print("set", s)
        input_samples = db.load('../00_Blender/%s_s1.h5' % s)
        assert np.max(input_samples['images']) > 50
        size = input_samples['alphas'][npslice].shape[0]

        min, max = range_extend_min_max(input_samples['images'], input_samples['alphas'])
        print("range_extend: min", min, "max", max)
        range_extended_images = range_extend(input_samples['images'][npslice],
                                             input_samples['alphas'][npslice], min, max)
        range_extended_db = {
            'images': range_extended_images,
            'alphas': input_samples['alphas'][npslice],
            'params': input_samples['params'][npslice],
            'label': np.full((size), i, dtype='uint8'),
            'label_split': np.full((size, len(sets)), [x == i for x in range(len(sets))], dtype='uint8')
        }

        bg = bgp.ImageNet(output_type)

        newdb = generate_dataset(range_extended_db, bg.generator(), size)
        newdb['background_indexes'] = np.array(bg.provided_indexes)

        imagenet.append(newdb)
        black.append(range_extended_db)

    db.write(imagenet_filename, append(imagenet))
    db.write(black_filename, append(black))


if __name__ == '__main__':
    random.seed(1)

    gen(np.s_[40000:], 'val', sys.argv[4], sys.argv[2])
    gen(np.s_[:40000], 'train', sys.argv[3], sys.argv[1])
